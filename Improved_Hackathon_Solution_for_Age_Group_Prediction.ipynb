{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niranjan0311/sa_2025/blob/main/Improved_Hackathon_Solution_for_Age_Group_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Starting improved hackathon solution script...\")\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('Train_Data.csv')\n",
        "    test_df = pd.read_csv('Test_Data.csv')\n",
        "    sample_submission_df = pd.read_csv('Sample_Submission.csv')\n",
        "    print(\"Initial data loading complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Make sure 'Train_Data.csv', 'Test_Data.csv', and 'Sample_Submission.csv' are in the same directory.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Train_Data.csv head:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nTest_Data.csv head:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Create copies to avoid modifying the original DataFrames\n",
        "train_df_processed = train_df.copy()\n",
        "test_df_processed = test_df.copy()\n",
        "\n",
        "# --- Data Preprocessing ---\n",
        "\n",
        "# 1. Handle Missing Values\n",
        "print(\"\\nHandling missing values...\")\n",
        "\n",
        "# Impute 'DIQ010' specifically with its mode, as it seems to be a discrete/categorical feature\n",
        "# Calculate mode from training data only to prevent data leakage\n",
        "diq010_mode_train = train_df_processed['DIQ010'].mode()[0]\n",
        "train_df_processed['DIQ010'].fillna(diq010_mode_train, inplace=True)\n",
        "test_df_processed['DIQ010'].fillna(diq010_mode_train, inplace=True)\n",
        "print(f\"Missing values in 'DIQ010' imputed with mode: {diq010_mode_train}\")\n",
        "\n",
        "\n",
        "# Separate features (X) and target (y) before general imputation and feature engineering\n",
        "X_train_raw = train_df_processed.drop('age_group', axis=1)\n",
        "y_train_raw = train_df_processed['age_group']\n",
        "\n",
        "# Identify numerical columns for median imputation (excluding 'SEQN' and 'DIQ010' from this specific imputer's fit)\n",
        "numerical_cols = X_train_raw.select_dtypes(include=np.number).columns.tolist()\n",
        "# Remove 'SEQN' and 'DIQ010' if they were accidentally included and already handled\n",
        "numerical_cols = [col for col in numerical_cols if col not in ['SEQN']]\n",
        "\n",
        "\n",
        "# Initialize the imputer with median strategy for remaining numerical features\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_imputed = pd.DataFrame(imputer_numerical.fit_transform(X_train_raw[numerical_cols]),\n",
        "                               columns=numerical_cols, index=X_train_raw.index)\n",
        "\n",
        "# Transform test data using the imputer fitted on training data\n",
        "test_df_imputed = pd.DataFrame(imputer_numerical.transform(test_df_processed[numerical_cols]),\n",
        "                               columns=numerical_cols, index=test_df_processed.index)\n",
        "\n",
        "# Recombine with DIQ010 (which was separately imputed) and SEQN (which will be dropped later)\n",
        "X_train_final = X_train_imputed.copy()\n",
        "X_train_final['DIQ010'] = train_df_processed['DIQ010']\n",
        "X_train_final['SEQN'] = train_df_processed['SEQN'] # Keep SEQN for now, drop later\n",
        "\n",
        "test_df_final = test_df_imputed.copy()\n",
        "test_df_final['DIQ010'] = test_df_processed['DIQ010']\n",
        "test_df_final['SEQN'] = test_df_processed['SEQN'] # Keep SEQN for now, drop later\n",
        "\n",
        "\n",
        "# For the 'age_group' target column, impute missing values with the mode\n",
        "y_train_imputed_series = y_train_raw.fillna(y_train_raw.mode()[0])\n",
        "print(\"Missing values in features imputed using median/mode strategy.\")\n",
        "\n",
        "\n",
        "# 2. Encode Target Variable\n",
        "# Map 'Adult' to 0 and 'Senior' to 1\n",
        "y_train_encoded = y_train_imputed_series.map({'Adult': 0, 'Senior': 1})\n",
        "print(\"Target variable 'age_group' encoded to 0 (Adult) and 1 (Senior).\")\n",
        "print(\"Encoded target value counts (before SMOTE):\")\n",
        "print(y_train_encoded.value_counts()) # Check imbalance\n",
        "\n",
        "\n",
        "# 3. Feature Engineering\n",
        "print(\"\\nPerforming feature engineering...\")\n",
        "epsilon = 1e-6 # Small value to prevent division by zero\n",
        "\n",
        "# Glucose to Insulin Ratio\n",
        "X_train_final['GLU_to_IN_Ratio'] = X_train_final['LBXGLU'] / (X_train_final['LBXIN'] + epsilon)\n",
        "test_df_final['GLU_to_IN_Ratio'] = test_df_final['LBXGLU'] / (test_df_final['LBXIN'] + epsilon)\n",
        "\n",
        "# Glucose Tolerance Difference\n",
        "X_train_final['Glucose_Tolerance_Diff'] = X_train_final['LBXGLT'] - X_train_final['LBXGLU']\n",
        "test_df_final['Glucose_Tolerance_Diff'] = test_df_final['LBXGLT'] - test_df_final['LBXGLU']\n",
        "\n",
        "# BMI and Physical Activity Interaction\n",
        "X_train_final['BMI_PAQ_Interaction'] = X_train_final['BMXBMI'] * X_train_final['PAQ605']\n",
        "test_df_final['BMI_PAQ_Interaction'] = test_df_final['BMXBMI'] * test_df_final['PAQ605']\n",
        "\n",
        "print(\"New features created: GLU_to_IN_Ratio, Glucose_Tolerance_Diff, BMI_PAQ_Interaction.\")\n",
        "\n",
        "# 4. Drop 'SEQN' column (identifier)\n",
        "X_train_final = X_train_final.drop('SEQN', axis=1)\n",
        "test_df_final = test_df_final.drop('SEQN', axis=1)\n",
        "print(\"'SEQN' column dropped from training and test feature sets.\")\n",
        "\n",
        "print(\"\\nPreprocessed training data info (with new features):\")\n",
        "print(X_train_final.info())\n",
        "print(\"\\nPreprocessed test data info (with new features):\")\n",
        "print(test_df_final.info())\n",
        "\n",
        "# --- Model Training ---\n",
        "\n",
        "# Split the training data into training and validation sets for evaluation\n",
        "# Stratify ensures that the proportion of 'Adult' and 'Senior' is maintained in splits\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_final, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded\n",
        ")\n",
        "print(\"\\nTraining data split into training (80%) and validation (20%) sets.\")\n",
        "print(f\"Original training split class distribution: {Counter(y_train_split)}\")\n",
        "\n",
        "# 5. Handle Class Imbalance with SMOTE on the training split\n",
        "print(\"Applying SMOTE to the training data to handle class imbalance...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_split, y_train_split)\n",
        "print(f\"Resampled training split class distribution: {Counter(y_train_smote)}\")\n",
        "\n",
        "\n",
        "# Initialize XGBoostClassifier\n",
        "# Calculate scale_pos_weight to balance classes for XGBoost directly\n",
        "# This gives more weight to the minority class\n",
        "neg_count = y_train_smote.value_counts()[0] # Count of majority class (Adult)\n",
        "pos_count = y_train_smote.value_counts()[1] # Count of minority class (Senior)\n",
        "scale_pos_weight_value = neg_count / pos_count\n",
        "print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight_value:.2f}\")\n",
        "\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',  # For binary classification\n",
        "    eval_metric='logloss',        # Metric for evaluation\n",
        "    use_label_encoder=False,      # Suppress warning\n",
        "    n_jobs=-1,                    # Use all available CPU cores\n",
        "    random_state=42,\n",
        "    scale_pos_weight=scale_pos_weight_value # Important for imbalance\n",
        ")\n",
        "\n",
        "# 6. Hyperparameter Tuning for XGBoost using RandomizedSearchCV\n",
        "print(\"\\nPerforming RandomizedSearchCV for hyperparameter tuning (this may take a moment)...\")\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'lambda': [0.5, 1, 1.5], # L2 regularization\n",
        "    'alpha': [0, 0.1, 0.2]   # L1 regularization\n",
        "}\n",
        "\n",
        "# n_iter controls how many different combinations are tried. Increase for more thorough search.\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50, # Number of parameter settings that are sampled.\n",
        "    scoring='f1', # Optimize for F1 score\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "print(f\"\\nBest XGBoost parameters found: {random_search.best_params_}\")\n",
        "print(\"XGBoostClassifier model trained with best parameters.\")\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "\n",
        "# Make predictions on the original (un-SMOTEd) validation set\n",
        "y_pred_val = best_xgb_model.predict(X_val_split)\n",
        "y_pred_proba_val = best_xgb_model.predict_proba(X_val_split)[:, 1] # Probabilities for positive class\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "f1 = f1_score(y_val_split, y_pred_val)\n",
        "print(f\"\\nF1 Score on the validation set: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report on Validation Set:\")\n",
        "print(classification_report(y_val_split, y_pred_val, target_names=['Adult', 'Senior']))\n",
        "\n",
        "print(\"\\nConfusion Matrix on Validation Set:\")\n",
        "print(confusion_matrix(y_val_split, y_pred_val))\n",
        "\n",
        "# You can also explore threshold tuning to potentially optimize F1 score further if needed\n",
        "# from sklearn.metrics import precision_recall_curve\n",
        "# precisions, recalls, thresholds = precision_recall_curve(y_val_split, y_pred_proba_val)\n",
        "# f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
        "# best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "# print(f\"Best threshold for F1: {best_threshold:.4f}\")\n",
        "# y_pred_val_tuned = (y_pred_proba_val >= best_threshold).astype(int)\n",
        "# f1_tuned = f1_score(y_val_split, y_pred_val_tuned)\n",
        "# print(f\"F1 Score on validation set with tuned threshold: {f1_tuned:.4f}\")\n",
        "\n",
        "\n",
        "# --- Prediction and Submission ---\n",
        "\n",
        "# Make predictions on the preprocessed test data\n",
        "test_predictions = best_xgb_model.predict(test_df_final)\n",
        "print(\"\\nPredictions made on the test dataset.\")\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'age_group': test_predictions\n",
        "})\n",
        "\n",
        "# Convert 0 back to 'Adult' and 1 back to 'Senior' for submission\n",
        "submission_df['age_group'] = submission_df['age_group'].map({0: 'Adult', 1: 'Senior'})\n",
        "\n",
        "# Save the submission file in the specified format\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully.\")\n",
        "print(\"Submission file head:\")\n",
        "print(submission_df.head())\n",
        "print(\"\\n--- Solution Completed ---\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting improved hackathon solution script...\n",
            "Initial data loading complete.\n",
            "Train_Data.csv head:\n",
            "      SEQN  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN age_group\n",
            "0  73564.0       2.0     2.0    35.7   110.0     2.0   150.0  14.91     Adult\n",
            "1  73568.0       2.0     2.0    20.3    89.0     2.0    80.0   3.85     Adult\n",
            "2  73576.0       1.0     2.0    23.2    89.0     2.0    68.0   6.14     Adult\n",
            "3  73577.0       1.0     2.0    28.9   104.0     NaN    84.0  16.15     Adult\n",
            "4  73580.0       2.0     1.0    35.9   103.0     2.0    81.0  10.92     Adult\n",
            "\n",
            "Test_Data.csv head:\n",
            "      SEQN  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN\n",
            "0  77017.0       1.0     1.0    32.2    96.0     2.0   135.0  15.11\n",
            "1  75580.0       2.0     2.0    26.3   100.0     2.0   141.0  15.26\n",
            "2  73820.0       1.0     2.0    28.6   107.0     2.0   136.0   8.82\n",
            "3  80489.0       2.0     1.0    22.1    93.0     2.0   111.0  12.13\n",
            "4  82047.0       1.0     1.0    24.7    91.0     2.0   105.0   3.12\n",
            "\n",
            "Handling missing values...\n",
            "Missing values in 'DIQ010' imputed with mode: 2.0\n",
            "Missing values in features imputed using median/mode strategy.\n",
            "Target variable 'age_group' encoded to 0 (Adult) and 1 (Senior).\n",
            "Encoded target value counts (before SMOTE):\n",
            "age_group\n",
            "0    1652\n",
            "1     314\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Performing feature engineering...\n",
            "New features created: GLU_to_IN_Ratio, Glucose_Tolerance_Diff, BMI_PAQ_Interaction.\n",
            "'SEQN' column dropped from training and test feature sets.\n",
            "\n",
            "Preprocessed training data info (with new features):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1966 entries, 0 to 1965\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   RIAGENDR                1966 non-null   float64\n",
            " 1   PAQ605                  1966 non-null   float64\n",
            " 2   BMXBMI                  1966 non-null   float64\n",
            " 3   LBXGLU                  1966 non-null   float64\n",
            " 4   DIQ010                  1966 non-null   float64\n",
            " 5   LBXGLT                  1966 non-null   float64\n",
            " 6   LBXIN                   1966 non-null   float64\n",
            " 7   GLU_to_IN_Ratio         1966 non-null   float64\n",
            " 8   Glucose_Tolerance_Diff  1966 non-null   float64\n",
            " 9   BMI_PAQ_Interaction     1966 non-null   float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 153.7 KB\n",
            "None\n",
            "\n",
            "Preprocessed test data info (with new features):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 312 entries, 0 to 311\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   RIAGENDR                312 non-null    float64\n",
            " 1   PAQ605                  312 non-null    float64\n",
            " 2   BMXBMI                  312 non-null    float64\n",
            " 3   LBXGLU                  312 non-null    float64\n",
            " 4   DIQ010                  312 non-null    float64\n",
            " 5   LBXGLT                  312 non-null    float64\n",
            " 6   LBXIN                   312 non-null    float64\n",
            " 7   GLU_to_IN_Ratio         312 non-null    float64\n",
            " 8   Glucose_Tolerance_Diff  312 non-null    float64\n",
            " 9   BMI_PAQ_Interaction     312 non-null    float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 24.5 KB\n",
            "None\n",
            "\n",
            "Training data split into training (80%) and validation (20%) sets.\n",
            "Original training split class distribution: Counter({0: 1321, 1: 251})\n",
            "Applying SMOTE to the training data to handle class imbalance...\n",
            "Resampled training split class distribution: Counter({0: 1321, 1: 1321})\n",
            "Calculated scale_pos_weight for XGBoost: 1.00\n",
            "\n",
            "Performing RandomizedSearchCV for hyperparameter tuning (this may take a moment)...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "\n",
            "Best XGBoost parameters found: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.1, 'lambda': 0.5, 'gamma': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}\n",
            "XGBoostClassifier model trained with best parameters.\n",
            "\n",
            "F1 Score on the validation set: 0.2909\n",
            "\n",
            "Classification Report on Validation Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Adult       0.86      0.91      0.88       331\n",
            "      Senior       0.34      0.25      0.29        63\n",
            "\n",
            "    accuracy                           0.80       394\n",
            "   macro avg       0.60      0.58      0.59       394\n",
            "weighted avg       0.78      0.80      0.79       394\n",
            "\n",
            "\n",
            "Confusion Matrix on Validation Set:\n",
            "[[300  31]\n",
            " [ 47  16]]\n",
            "\n",
            "Predictions made on the test dataset.\n",
            "\n",
            "Submission file 'submission.csv' created successfully.\n",
            "Submission file head:\n",
            "  age_group\n",
            "0     Adult\n",
            "1     Adult\n",
            "2     Adult\n",
            "3     Adult\n",
            "4     Adult\n",
            "\n",
            "--- Solution Completed ---\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT01iwLYVnC0",
        "outputId": "9f5bd582-da1b-4cf5-cde0-01e07026a848"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}